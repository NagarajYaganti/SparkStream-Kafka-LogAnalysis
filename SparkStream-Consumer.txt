package Bigdata.Spak_Kafka

import org.apache.spark.sql.SparkSession
import java.util.regex.Pattern
import java.util.regex.Matcher
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.sql.SQLContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe


/**
 * The LogAnalyzer takes in an apache access log file from kafka and 
 * computes ip 
 *
 * a1: ClintIP, a2:clientIdentity, a3:UserName, a4:dateTime, a5:request, a6:statusCode, a7:bytesSent 
 * a8: referer, a9:userAgent 
 *   
 */

case class AccessLg(a1: String, a2: String, a3: String, a4: String, a5:String, a6:Int, a7:Int, a8:String, a9:String ) extends Serializable {
  
}
object Naga1
{
  val c1 = "\\d{1,3}"                      
  val b1 = s"($c1\\.$c1\\.$c1\\.$c1)?"  
  val b2 = "(\\S+)"                     
  val b3 = "(\\S+)"
  val b4 = "(\\[.+?\\])"              
  val b5 = "\"(.*?)\""                 
  val b6 = "(\\d{3})"
  val b7 = "(\\S+)"                      
  val b8 = "\"(.*?)\""
  val b9 = "\"(.*?)\""
  val reg = s"$b1 $b2 $b3 $b4 $b5 $b6 $b7 $b8 $b9"
  
  val x = Pattern.compile(reg)
  
  def parseRec(record: String): AccessLg = 
    {
        val matcher = x.matcher(record)
        if (matcher.find) {
            buildAccessLg(matcher)
        } else {
            AccessLg("", "", "", "", "", 1, 1, "", "")
        }
    }
  
  def buildAccessLg(matcher: Matcher) = {
        AccessLg(
            matcher.group(1),
            matcher.group(2),
            matcher.group(3),
            matcher.group(4),
            matcher.group(5),
            matcher.group(6).toInt,
            matcher.group(7).toInt,
            matcher.group(8),
            matcher.group(9))
   }
  def main(args: Array[String]) 
   {
     val conf = new SparkConf().setAppName("Nsample").setMaster("local[*]")
     val sc = new SparkContext(conf)
     val stream = new StreamingContext(sc, Seconds(10))
     
     val kafkaParams = Map[String, Object](
        "bootstrap.servers" -> "localhost:9092",
        "key.deserializer" -> classOf[StringDeserializer],
        "value.deserializer" -> classOf[StringDeserializer],
        "group.id" -> "app2")
        
     val topics = Array("Ntopic") 
    
     val lins = KafkaUtils.createDirectStream[String, String](
        stream,
        PreferConsistent,
        Subscribe[String, String](topics, kafkaParams))
                   
     val rec = lins.map(_.value)
      
     rec.foreachRDD(rdd => {
         val spark = SparkSession
                .builder()
                .config(rdd.sparkContext.getConf)
                .getOrCreate() 
          import spark.implicits._
     
          val df = rdd.map(parseRec).toDF()
          df.createOrReplaceTempView("nlog")             
          val res = spark.sql("select a1 count(*) as count from nlog where a6 > 400 GROUP BY a1 HAVING count > 10)
          res.saveAsTextFiles("C:/Naga/DDOSIPS") 
          res.show()
     })
     
     stream.start()
     stream.awaitTermination()
   }
}